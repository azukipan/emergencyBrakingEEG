{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code produces an EEG dataset according to the methodology described by Haufe et al. in \"EEG potentials predict upcoming emergency brakings during simulated driving.\" dx.doi.org/10.1088/1741-2560/8/5/056001. Instead of the original modeling approach, we train and test our own model here. \n",
    "\n",
    "The original EEG data are part of a dataset from Haufe et al. called \"Emergency braking during simulated driving.\" The dataset is available at http://bnci-horizon-2020.eu/database/data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "f = h5py.File('../../EEG_Dataset_Haufe/VPae.mat','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = f.get('cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sort the data and assign to variables for: \n",
    "\n",
    "signal channel names: cnt.clab\n",
    "sampling frequency: cnt.fs\n",
    "time-series data: cnt.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt.clab = np.array(cnt['clab'])\n",
    "cnt.fs = np.array(cnt['fs'])\n",
    "cnt.x = np.array(cnt['x']) \n",
    "\n",
    "samplingRate = cnt.fs[0][0] #Down-/upsample rate for all data = 200Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the names of the signal channels, along with the index of one of the channels. The index indicates to the order of the corresponding data array in cnt.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fz  index = 10\n",
      "EOGv,Fp1,Fp2,AF3,AF4,EOGh,F7,F5,F3,F1,Fz,F2,F4,F6,F8,FT7,FC5,FC3,FC1,FCz,FC2,FC4,FC6,FT8,T7,C5,C3,C1,Cz,C2,C4,C6,T8,TP7,CP5,CP3,CP1,CPz,CP2,CP4,CP6,TP8,P9,P7,P5,P3,P1,Pz,P2,P4,P6,P8,P10,PO7,PO3,POz,PO4,PO8,O1,Oz,O2,EMGf,lead_gas,lead_brake,dist_to_lead,wheel_X,wheel_Y,gas,brake\n"
     ]
    }
   ],
   "source": [
    "channelNames = ''\n",
    "channelOfInterest = 'Fz'\n",
    "\n",
    "for i in range(0,len(cnt.clab)):\n",
    "    ref = cnt.clab[i][0]\n",
    "    res = cnt[ref]\n",
    "    #print(bytes(np.array(res).ravel().tolist()).decode('UTF-8'))\n",
    "    channelName = bytes(np.array(res).ravel().tolist()).decode('UTF-8')\n",
    "    if channelName == channelOfInterest : #'EMGf':\n",
    "        print(channelOfInterest,' index =',i)\n",
    "    channelNames = channelNames + channelName\n",
    "    if i < len(cnt.clab)-1:\n",
    "        channelNames = channelNames + ','\n",
    "print(channelNames)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the data for the channel of interest. Note that the data are a concatenation of three 45-minute blocks collected from given test subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data, channelName, units = cnt.x[0], 'EOGv', 'unknown'\n",
    "#data, channelName, units = cnt.x[61], 'EMGf', 'unknown' \n",
    "#data, channelName, units = cnt.x[68], 'Brake', 'unknown' \n",
    "#data, channelName, units = cnt.x[66], 'Y-axis Wheel', 'unknown' \n",
    "#data, channelName, units = cnt.x[65], 'X-axis Wheel', 'unknown'  \n",
    "#data, channelName, units = cnt.x[64], 'Inter-vehicle Distance',  'm'  \n",
    "data, channelName, units = cnt.x[10], 'Fz', 'unknown'\n",
    "\n",
    "numDataPoints = len(data) \n",
    "t = np.arange(0, numDataPoints/samplingRate, 1/samplingRate)\n",
    "#print('Number of data points = ' + str(numDataPoints))\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(t, data)\n",
    "ax.set_ylabel(units)\n",
    "ax.set_xlabel('Seconds')\n",
    "ax.set_title('Channel ' + channelName)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll read the event data corresponding to the experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrk = f.get('mrk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the event data consists of: \n",
    "\n",
    "\"class names\": Categories of events\n",
    "\"event\": events contains a key called \"react,\" which contains reaction times between between car_brake and react_emg.\n",
    "\"time\": timestamps for each event.\n",
    "\"y\": indicates which class an event belongs to for a given timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['className', 'event', 'time', 'y']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrk.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read the different event data into specified variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrk.classNames = mrk['className']\n",
    "mrk.time = mrk['time']\n",
    "mrk.y = mrk['y']\n",
    "mrk.events = mrk['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Print brake reaction times of driver.\\nmrk.events['react'][0]\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Print brake reaction times of driver.\n",
    "mrk.events['react'][0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"time\": shape (914, 1), type \"<f8\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrk.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_normal,car_brake,car_hold,car_collision,react_emg\n"
     ]
    }
   ],
   "source": [
    "classNames = ''\n",
    "ref = mrk.classNames[4][0]\n",
    "res = mrk[ref]\n",
    "\n",
    "for i in range(0, len(mrk.classNames)):\n",
    "    ref = mrk.classNames[i][0]\n",
    "    res = mrk[ref]\n",
    "    className = bytes(np.array(res).ravel().tolist()).decode('UTF-8')\n",
    "    classNames = classNames + className\n",
    "    if i < len(mrk.classNames)-1:\n",
    "        classNames = classNames + ','\n",
    "print(classNames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Print event times\\nfor i in range(0, len(mrk.time)):\\n    print(mrk.time[i][0])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Print event times\n",
    "for i in range(0, len(mrk.time)):\n",
    "    print(mrk.time[i][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Print event category values: 0 = no event, 1 = event\\nmrk.y\\nfor i in range(0, len(mrk.y)):\\n    print(mrk.y[i][1])\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Print event category values: 0 = no event, 1 = event\n",
    "mrk.y\n",
    "for i in range(0, len(mrk.y)):\n",
    "    print(mrk.y[i][1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all car braking events and store corresponding timestamps.\n",
    "carBrakeTime = []\n",
    "\n",
    "for i in range(0, len(mrk.y)):\n",
    "    #print( mrk.y[i][1])\n",
    "    if mrk.y[i][1] == 1: #Check if car is braking, i.e. y[i] = 1\n",
    "        carBrakeTime.append(mrk.time[i][0]) #Store timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "#Create training data from braking event EEG via these steps:\n",
    "#Get segments of braking event EEG.\n",
    "#Covert to PSD.\n",
    "#Store first 4 PSD components of each segment in variable for training.\n",
    "def createDatasetFromEEGEvents(timestamps, data, samplingRate, numberOfPSDComponents = 4):\n",
    "    dt = 1/samplingRate #Time increment in seconds\n",
    "    \n",
    "    dt1_index = 0\n",
    "    dt2_index = int(100/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint at 100 ms.\n",
    "    baselineCorrection_eeg = mean(data[dt1_index:dt2_index+1])\n",
    "    \n",
    "    #Define variables to split data in first 1/2 for training and second 1/2 for validation\n",
    "    brakingEvent_eeg_PSD_train = []\n",
    "    brakingEvent_eeg_PSD_val = []\n",
    "\n",
    "    dt = 1/samplingRate #Time increment in seconds\n",
    "\n",
    "    for time in carBrakeTime: #Iterate through event timestamps in milliseconds\n",
    "        #index = int(time/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint\n",
    "        dt1_index = int((time-300)/1000/dt) #Index of datapoint 300 ms before event datapoint.\n",
    "        dt2_index = int((time+1200)/1000/dt) #Index of datapoint 1200 ms before event datapoint.\n",
    "\n",
    "        brakingEvent_eeg = data[dt1_index:dt2_index+1]-baselineCorrection_eeg\n",
    "        #Normalize signal data WRT to max and find generate power spectral density \n",
    "        freq_data, time_data, pwr_spectral_density_data = spectrogram(\n",
    "                                                            np.array([brakingEvent_eeg/max(brakingEvent_eeg)]), \n",
    "                                                            samplingRate\n",
    "                                                            )\n",
    "        if time < len(data)*1000*dt/2:\n",
    "            brakingEvent_eeg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "            continue\n",
    "        brakingEvent_eeg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "        \n",
    "    return brakingEvent_eeg_PSD_train, brakingEvent_eeg_PSD_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "#Create baseline training EEG data containing no braking event EEG via these steps:\n",
    "#Get 100 ms EEG segment at beginning of data to use for baseline correction.\n",
    "#Get segments of EEG without braking events and subract 100 ms EEG segment.\n",
    "#Covert to PSD.\n",
    "#Store first 4 PSD components of each segment in variable for training.\n",
    "\n",
    "def createDatasetFromEEGWithoutEvents(timestamps, data, samplingRate, numberOfPSDComponents=4):\n",
    "    dt = 1/samplingRate #Time increment in seconds\n",
    "    \n",
    "    dt1_index = 0\n",
    "    dt2_index = int(100/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint at 100 ms.\n",
    "    baselineCorrection_eeg = mean(data[dt1_index:dt2_index+1])\n",
    "\n",
    "    noEvent_eeg_PSD_train = []\n",
    "    noEvent_eeg_PSD_val = []\n",
    "\n",
    "    for i in range(0, len(mrk.time)): #Iterate through all event timestamps in milliseconds\n",
    "        if mrk.time[i][0] < 4500: #Skip iteration if there is not enough time to get an EEG segment between time of first datapoint and time of first event. \n",
    "            continue\n",
    "\n",
    "        if i > 0:\n",
    "            if mrk.time[i][0]-mrk.time[i-1][0] < 7500: #Skip iteration if there is not enough time to get EEG segment between current and previous timestamps.\n",
    "                continue\n",
    "\n",
    "        numberOfSegments = int((mrk.time[i][0]-mrk.time[i-1][0]-6000)/2000) #Calculate how many user-specified EEG segments can fit between two events.\n",
    "\n",
    "        for segmentNum in range(0, numberOfSegments):\n",
    "            #Add 500 ms between each EEG segment, except for segment closest in time to event\n",
    "            dt1_index = int((mrk.time[i][0]-5000+(2000*segmentNum)-500)/1000/dt) #500 represents 500 ms offset between each EEG segment.\n",
    "            dt2_index = int((mrk.time[i][0]-3000+(2000*segmentNum))/1000/dt)\n",
    "\n",
    "            #dt1_index = int((mrk.time[i][0]-4500)/1000/dt)\n",
    "            #dt2_index = int((mrk.time[i][0]-3000)/1000/dt)\n",
    "            noEvent_eeg = data[dt1_index:dt2_index+1]-baselineCorrection_eeg #Get EEG segment immediately prior to current event.\n",
    "            #Normalize signal data WRT to max and find generate power spectral density \n",
    "            freq_data, time_data, pwr_spectral_density_data = spectrogram( \n",
    "                                                                np.array([noEvent_eeg/max(noEvent_eeg)]), \n",
    "                                                                samplingRate\n",
    "                                                                )\n",
    "            if mrk.time[i][0] < len(data)*1000*dt/2:\n",
    "                noEvent_eeg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "                continue\n",
    "            noEvent_eeg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "    \n",
    "        if i == len(mrk.time): #If iteration reaches last event timestamp, set indices to get any possible EEG segment beyond timestamp.\n",
    "            numberOfSegments = int((len(data)*1000*dt/2-mrk.time[i][0])/2000) #Calculate how many user-specified EEG segments can fit between two events.\n",
    "\n",
    "            for segmentNum in range(0, numberOfSegments):\n",
    "                #Add 500 ms between each EEG segment, except for segment closest in time to event\n",
    "                dt1_index = int((mrk.time[i][0]+3000+(2000*segmentNum))/1000/dt)\n",
    "                dt2_index = int((mrk.time[i][0]+5000+(2000*segmentNum)-500)/1000/dt) #500 represents 500 ms offset between each EEG segment.\n",
    "\n",
    "\n",
    "                #dt1_index = int((mrk.time[i][0]+3000)/1000/dt)\n",
    "                #dt2_index = int((mrk.time[i][0]+4500)/1000/dt)\n",
    "                #if dt2_index > len(data)-1: #If the second index is greater than index of last EEG datapoint, skip iteration.\n",
    "                #    continue\n",
    "                noEvent_eeg = data[dt1_index:dt2_index+1]  #Get EEG segment \n",
    "                #Normalize signal data WRT to max and find generate power spectral density \n",
    "                freq_data, time_data, pwr_spectral_density_data = spectrogram( \n",
    "                                                                    np.array([noEvent_eeg/max(noEvent_eeg)]), \n",
    "                                                                    samplingRate\n",
    "                                                                    )\n",
    "                if mrk.time[i][0] < len(data)*1000*dt/2:\n",
    "                    noEvent_eeg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "                    continue\n",
    "                noEvent_eeg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "    \n",
    "    return noEvent_eeg_PSD_train, noEvent_eeg_PSD_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from statistics import mean\n",
    "#Create baseline training EEG data containing no braking event EEG via these steps:\n",
    "#Get 100 ms EEG segment at beginning of data to use for baseline correction.\n",
    "#Get segments of EEG without braking events and subract 100 ms EEG segment.\n",
    "#Covert to PSD.\n",
    "#Store first 4 PSD components of each segment in variable for training.\n",
    "numberOfPSDComponents = 4\n",
    "\n",
    "dt = 1/samplingRate #Time increment in seconds\n",
    "\n",
    "dt1_index = 0\n",
    "dt2_index = int(100/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint at 100 ms.\n",
    "baselineCorrection_eeg = mean(data[dt1_index:dt2_index+1])\n",
    "\n",
    "noEvent_eeg_PSD_train = []\n",
    "noEvent_eeg_PSD_val = []\n",
    "    \n",
    "for i in range(0, len(mrk.time)): #Iterate through all event timestamps in milliseconds\n",
    "    if mrk.time[i][0] < 4500: #Skip iteration if there is not enough time to get an EEG segment between time of first datapoint and time of first event. \n",
    "        continue\n",
    "    \n",
    "    if i > 0:\n",
    "        if mrk.time[i][0]-mrk.time[i-1][0] < 7500: #Skip iteration if there is not enough time to get EEG segment between current and previous timestamps.\n",
    "            continue\n",
    "    dt1_index = int((mrk.time[i][0]-4500)/1000/dt)\n",
    "    dt2_index = int((mrk.time[i][0]-3000)/1000/dt)\n",
    "    noEvent_eeg = data[dt1_index:dt2_index+1]-baselineCorrection_eeg  #Get EEG segment immediately prior to current event.\n",
    "    #Normalize signal data WRT to max and find generate power spectral density \n",
    "    freq_data, time_data, pwr_spectral_density_data = spectrogram( \n",
    "                                                        np.array([noEvent_eeg/max(noEvent_eeg)]), \n",
    "                                                        samplingRate\n",
    "                                                        )\n",
    "    if mrk.time[i][0] < len(data)*1000*dt/2:\n",
    "        noEvent_eeg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "        continue\n",
    "    noEvent_eeg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "    if i == len(mrk.time): #If iteration reaches last event timestamp, set indices to get any possible EEG segment beyond timestamp.\n",
    "        dt1_index = int((mrk.time[i][0]+3000)/1000/dt)\n",
    "        dt2_index = int((mrk.time[i][0]+4500)/1000/dt)\n",
    "        if dt2_index > len(data)-1: #If the second index is greater than index of last EEG datapoint, skip iteration.\n",
    "            continue\n",
    "        noEvent_eeg = data[dt1_index:dt2_index+1]  #Get EEG segment immediately prior to current event.\n",
    "        #Normalize signal data WRT to max and find generate power spectral density \n",
    "        freq_data, time_data, pwr_spectral_density_data = spectrogram( \n",
    "                                                            np.array([noEvent_eeg/max(noEvent_eeg)]), \n",
    "                                                            samplingRate\n",
    "                                                            )\n",
    "        if mrk.time[i][0] < len(data)*1000*dt/2:\n",
    "            noEvent_eeg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "            continue\n",
    "        noEvent_eeg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "#Create train and validation datasets\n",
    "\n",
    "numberOfPSDComponents = 4\n",
    "\n",
    "brakingEvent_eeg_PSD_train = []\n",
    "brakingEvent_eeg_PSD_val = []\n",
    "\n",
    "'''\n",
    "noEvent_eeg_PSD_train = []\n",
    "noEvent_eeg_PSD_val = []\n",
    "'''\n",
    "\n",
    "#Iterate through all EEG channels.\n",
    "for channelNum in tqdm(range(0, 61)):\n",
    "    \n",
    "    #P9, P10, CPz, FCz\n",
    "    channelsOfInterest = ['P9', 'P10', 'CPz', 'FCz'] <-- 20% accuracy\n",
    "    #['P9'] <-- 48% accuracy\n",
    "    #['POz','FCz'] <-- 34%\n",
    "    #['FCz'] <-- 42% accuracy\n",
    "    #['F3', 'F4', 'C3', 'C4'] <-- 19% accuracy\n",
    "    #['POz'] <-- 44% accuracy\n",
    "    #['O1', 'O2', 'Oz'] <-- 25% accuracy\n",
    "    #['POz']#['FC3', 'FC4', 'Fz']#['Fz', 'FCz']\n",
    "    ref = cnt.clab[channelNum][0]\n",
    "    #print(bytes(np.array(res).ravel().tolist()).decode('UTF-8'))\n",
    "    channelName = bytes(np.array(cnt[ref]).ravel().tolist()).decode('UTF-8')\n",
    "    \n",
    "    if channelName in channelsOfInterest:\n",
    "    #if channelNum not in [0,3,4,5]:\n",
    "        data = cnt.x[channelNum]\n",
    "        event_eeg_PSD_train, event_eeg_PSD_val = createDatasetFromEEGEvents(mrk.time, data, samplingRate, numberOfPSDComponents)\n",
    "        \n",
    "        #_noEvent_eeg_PSD_train, _noEvent_eeg_PSD_val = createDatasetFromEEGWithoutEvents(mrk.time, data, samplingRate, numberOfPSDComponents)\n",
    "        \n",
    "        for array in event_eeg_PSD_train:\n",
    "            #brakingEvent_eeg_PSD_train = np.concatenate((brakingEvent_eeg_PSD_train, array))\n",
    "            brakingEvent_eeg_PSD_train.append(array)\n",
    "        for array in event_eeg_PSD_val:\n",
    "            #brakingEvent_eeg_PSD_val = np.concatenate((brakingEvent_eeg_PSD_val, array))\n",
    "            brakingEvent_eeg_PSD_val.append(array)\n",
    "            \n",
    "        '''\n",
    "        for array in _noEvent_eeg_PSD_train:\n",
    "            #noEvent_eeg_PSD_train = np.concatenate((noEvent_eeg_PSD_train, array))\n",
    "            noEvent_eeg_PSD_train.append(array)\n",
    "        for array in _noEvent_eeg_PSD_val:\n",
    "            #noEvent_eeg_PSD_val = np.concatenate((noEvent_eeg_PSD_val, array))\n",
    "            noEvent_eeg_PSD_val.append(array)\n",
    "        '''\n",
    "    \n",
    "#Label = 0 indicates no event; label = 1 indicates EEG braking event\n",
    "trainData = np.concatenate((brakingEvent_eeg_PSD_train, noEvent_eeg_PSD_train))\n",
    "trainLabels_event = np.ones(len(brakingEvent_eeg_PSD_train),dtype=int) \n",
    "trainLabels_noEvent = np.zeros(len(noEvent_eeg_PSD_train),dtype=int) \n",
    "trainLabels = np.concatenate((trainLabels_event, trainLabels_noEvent))\n",
    "\n",
    "valData = np.concatenate((brakingEvent_eeg_PSD_val, noEvent_eeg_PSD_val))\n",
    "valLabels_event = np.ones(len(brakingEvent_eeg_PSD_val),dtype=int) \n",
    "valLabels_noEvent = np.zeros(len(noEvent_eeg_PSD_val),dtype=int) \n",
    "valLabels = np.concatenate((valLabels_event, valLabels_noEvent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 61/61 [02:32<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#Create train and validation datasets\n",
    "\n",
    "numberOfPSDComponents = 1000 #Increasing components from 0 to 100 AUC accuracy.\n",
    "\n",
    "brakingEvent_eeg_PSD_train = []\n",
    "brakingEvent_eeg_PSD_val = []\n",
    "\n",
    "noEvent_eeg_PSD_train = []\n",
    "noEvent_eeg_PSD_val = []\n",
    "\n",
    "#Iterate through all EEG channels.\n",
    "for channelNum in tqdm(range(0, 61)):\n",
    "    \n",
    "    channelsOfInterest = ['P9', 'P10', 'CPz', 'FCz']#['O1', 'O2', 'Oz']#['POz']   <-- 46% accuracy\n",
    "    #['POz']#['FC3', 'FC4', 'Fz']#['Fz', 'FCz']#['F3', 'F4', 'C3', 'C4']\n",
    "    ref = cnt.clab[channelNum][0]\n",
    "    #print(bytes(np.array(res).ravel().tolist()).decode('UTF-8'))\n",
    "    channelName = bytes(np.array(cnt[ref]).ravel().tolist()).decode('UTF-8')\n",
    "    '''\n",
    "    if channelName in channelsOfInterest:\n",
    "    #if channelNum not in [0,3,4,5]:\n",
    "        data = cnt.x[channelNum]\n",
    "        event_eeg_PSD_train, event_eeg_PSD_val = createDatasetFromEEGEvents(mrk.time, data, samplingRate, numberOfPSDComponents)\n",
    "        _noEvent_eeg_PSD_train, _noEvent_eeg_PSD_val = createDatasetFromEEGWithoutEvents(mrk.time, data, samplingRate, numberOfPSDComponents)\n",
    "        \n",
    "        for array in event_eeg_PSD_train:\n",
    "            #brakingEvent_eeg_PSD_train = np.concatenate((brakingEvent_eeg_PSD_train, array))\n",
    "            brakingEvent_eeg_PSD_train.append(array)\n",
    "        for array in event_eeg_PSD_val:\n",
    "            #brakingEvent_eeg_PSD_val = np.concatenate((brakingEvent_eeg_PSD_val, array))\n",
    "            brakingEvent_eeg_PSD_val.append(array)\n",
    "        for array in _noEvent_eeg_PSD_train:\n",
    "            #noEvent_eeg_PSD_train = np.concatenate((noEvent_eeg_PSD_train, array))\n",
    "            noEvent_eeg_PSD_train.append(array)\n",
    "        for array in _noEvent_eeg_PSD_val:\n",
    "            #noEvent_eeg_PSD_val = np.concatenate((noEvent_eeg_PSD_val, array))\n",
    "            noEvent_eeg_PSD_val.append(array)\n",
    "    '''\n",
    "    data = cnt.x[channelNum]\n",
    "    event_eeg_PSD_train, event_eeg_PSD_val = createDatasetFromEEGEvents(mrk.time, data, samplingRate, numberOfPSDComponents)\n",
    "    _noEvent_eeg_PSD_train, _noEvent_eeg_PSD_val = createDatasetFromEEGWithoutEvents(mrk.time, data, samplingRate, numberOfPSDComponents)\n",
    "\n",
    "    for array in event_eeg_PSD_train:\n",
    "        #brakingEvent_eeg_PSD_train = np.concatenate((brakingEvent_eeg_PSD_train, array))\n",
    "        brakingEvent_eeg_PSD_train.append(array)\n",
    "    for array in event_eeg_PSD_val:\n",
    "        #brakingEvent_eeg_PSD_val = np.concatenate((brakingEvent_eeg_PSD_val, array))\n",
    "        brakingEvent_eeg_PSD_val.append(array)\n",
    "    for array in _noEvent_eeg_PSD_train:\n",
    "        #noEvent_eeg_PSD_train = np.concatenate((noEvent_eeg_PSD_train, array))\n",
    "        noEvent_eeg_PSD_train.append(array)\n",
    "    for array in _noEvent_eeg_PSD_val:\n",
    "        #noEvent_eeg_PSD_val = np.concatenate((noEvent_eeg_PSD_val, array))\n",
    "        noEvent_eeg_PSD_val.append(array)\n",
    "    \n",
    "#Label = 0 indicates no event; label = 1 indicates EEG braking event\n",
    "trainData = np.concatenate((brakingEvent_eeg_PSD_train, noEvent_eeg_PSD_train))\n",
    "trainLabels_event = np.ones(len(brakingEvent_eeg_PSD_train),dtype=int) \n",
    "trainLabels_noEvent = np.zeros(len(noEvent_eeg_PSD_train),dtype=int) \n",
    "trainLabels = np.concatenate((trainLabels_event, trainLabels_noEvent))\n",
    "\n",
    "valData = np.concatenate((brakingEvent_eeg_PSD_val, noEvent_eeg_PSD_val))\n",
    "valLabels_event = np.ones(len(brakingEvent_eeg_PSD_val),dtype=int) \n",
    "valLabels_noEvent = np.zeros(len(noEvent_eeg_PSD_val),dtype=int) \n",
    "valLabels = np.concatenate((valLabels_event, valLabels_noEvent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train LDA model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(np.array(trainData), trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate LDA model\n",
    "eventPredictions = clf.predict(valData).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model accuracy:  0.07\n"
     ]
    }
   ],
   "source": [
    "accuracy = abs(sum(eventPredictions-valLabels))/len(eventPredictions)\n",
    "print(\"LDA model accuracy: \",round(accuracy,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model AUC accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "#Area under the curve score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "accuracyAUC = roc_auc_score(valLabels, clf.predict_proba(np.array(valData))[:, 1])\n",
    "print(\"LDA model AUC accuracy: \",round(accuracyAUC,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo:\n",
    "#Try prediction with mutliple test subjects\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
